{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de lancer ce NB, je lance le terminal Anaconda et je copie colle : \n",
    "mlflow server --host 127.0.0.1 --port 8080 \n",
    "\n",
    "Ca lance MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour ce projet, nous avons suivi le schéma présenté dans cet article:\n",
    "https://towardsdatascience.com/a-complete-machine-learning-walk-through-in-python-part-one-c62152f39420"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d632b08c-d252-4238-b496-e2c6edebec4b",
    "_uuid": "eb13bf76d4e1e60d0703856ec391cdc2c5bdf1fb"
   },
   "source": [
    "# 1 - Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "# File system manangement\n",
    "import os\n",
    "\n",
    "# Suppress warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# sklearn preprocessing for dealing with categorical variables\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, f1_score, fbeta_score, recall_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from mlflow.models import infer_signature\n",
    "from lightgbm import early_stopping\n",
    "\n",
    "import shap\n",
    "# Memory management\n",
    "import gc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# démarrage du tracking\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration de la connexion MLflow\n",
    "mlflow.utils.rest_utils.DEFAULT_RETRIES = 10\n",
    "mlflow.utils.rest_utils.DEFAULT_BACKOFF_FACTOR = 0.2\n",
    "mlflow.utils.rest_utils.DEFAULT_TIMEOUT = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le serveur MLflow est accessible.\n"
     ]
    }
   ],
   "source": [
    "# Vérifiez que le serveur MLflow est accessible\n",
    "try:\n",
    "    mlflow.get_experiment_by_name(\"MLflow Credit_Scoring - Projet_7\")\n",
    "    print(\"Le serveur MLflow est accessible.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur de connexion au serveur MLflow: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reduced=pd.read_csv('train_reduced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EXT_SOURCE_1</th>\n",
       "      <th>CREDIT_TERM</th>\n",
       "      <th>EXT_SOURCE_2</th>\n",
       "      <th>EXT_SOURCE_3</th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>client_installments_AMT_PAYMENT_min_sum</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>bureau_DAYS_CREDIT_max</th>\n",
       "      <th>bureau_DAYS_CREDIT_ENDDATE_max</th>\n",
       "      <th>...</th>\n",
       "      <th>client_credit_CNT_DRAWINGS_POS_CURRENT_max_sum</th>\n",
       "      <th>client_credit_AMT_PAYMENT_CURRENT_min_sum</th>\n",
       "      <th>previous_NAME_SELLER_INDUSTRY_Industry_mean</th>\n",
       "      <th>client_credit_AMT_INST_MIN_REGULARITY_max_sum</th>\n",
       "      <th>previous_NAME_GOODS_CATEGORY_Sport and Leisure_mean</th>\n",
       "      <th>client_credit_CNT_DRAWINGS_POS_CURRENT_min_sum</th>\n",
       "      <th>client_credit_NAME_CONTRACT_STATUS_Completed_mean_min</th>\n",
       "      <th>OCCUPATION_TYPE_Drivers</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.083037</td>\n",
       "      <td>0.060749</td>\n",
       "      <td>0.262949</td>\n",
       "      <td>0.139376</td>\n",
       "      <td>-637.0</td>\n",
       "      <td>9461</td>\n",
       "      <td>175783.73</td>\n",
       "      <td>24700.5</td>\n",
       "      <td>-103.0</td>\n",
       "      <td>780.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>100002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.311267</td>\n",
       "      <td>0.027598</td>\n",
       "      <td>0.622246</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1188.0</td>\n",
       "      <td>16765</td>\n",
       "      <td>1154108.20</td>\n",
       "      <td>35698.5</td>\n",
       "      <td>-606.0</td>\n",
       "      <td>1216.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>100003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.555912</td>\n",
       "      <td>0.729567</td>\n",
       "      <td>-225.0</td>\n",
       "      <td>19046</td>\n",
       "      <td>16071.75</td>\n",
       "      <td>6750.0</td>\n",
       "      <td>-408.0</td>\n",
       "      <td>-382.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>100004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.094941</td>\n",
       "      <td>0.650442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3039.0</td>\n",
       "      <td>19005</td>\n",
       "      <td>994476.70</td>\n",
       "      <td>29686.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>100006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.042623</td>\n",
       "      <td>0.322738</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3038.0</td>\n",
       "      <td>19932</td>\n",
       "      <td>483756.38</td>\n",
       "      <td>21865.5</td>\n",
       "      <td>-1149.0</td>\n",
       "      <td>-783.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>100007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 349 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   EXT_SOURCE_1  CREDIT_TERM  EXT_SOURCE_2  EXT_SOURCE_3  DAYS_EMPLOYED  \\\n",
       "0      0.083037     0.060749      0.262949      0.139376         -637.0   \n",
       "1      0.311267     0.027598      0.622246           NaN        -1188.0   \n",
       "2           NaN     0.050000      0.555912      0.729567         -225.0   \n",
       "3           NaN     0.094941      0.650442           NaN        -3039.0   \n",
       "4           NaN     0.042623      0.322738           NaN        -3038.0   \n",
       "\n",
       "   DAYS_BIRTH  client_installments_AMT_PAYMENT_min_sum  AMT_ANNUITY  \\\n",
       "0        9461                                175783.73      24700.5   \n",
       "1       16765                               1154108.20      35698.5   \n",
       "2       19046                                 16071.75       6750.0   \n",
       "3       19005                                994476.70      29686.5   \n",
       "4       19932                                483756.38      21865.5   \n",
       "\n",
       "   bureau_DAYS_CREDIT_max  bureau_DAYS_CREDIT_ENDDATE_max  ...  \\\n",
       "0                  -103.0                           780.0  ...   \n",
       "1                  -606.0                          1216.0  ...   \n",
       "2                  -408.0                          -382.0  ...   \n",
       "3                     NaN                             NaN  ...   \n",
       "4                 -1149.0                          -783.0  ...   \n",
       "\n",
       "   client_credit_CNT_DRAWINGS_POS_CURRENT_max_sum  \\\n",
       "0                                             NaN   \n",
       "1                                             NaN   \n",
       "2                                             NaN   \n",
       "3                                             0.0   \n",
       "4                                             NaN   \n",
       "\n",
       "   client_credit_AMT_PAYMENT_CURRENT_min_sum  \\\n",
       "0                                        NaN   \n",
       "1                                        NaN   \n",
       "2                                        NaN   \n",
       "3                                        0.0   \n",
       "4                                        NaN   \n",
       "\n",
       "   previous_NAME_SELLER_INDUSTRY_Industry_mean  \\\n",
       "0                                          0.0   \n",
       "1                                          0.0   \n",
       "2                                          0.0   \n",
       "3                                          0.0   \n",
       "4                                          0.0   \n",
       "\n",
       "   client_credit_AMT_INST_MIN_REGULARITY_max_sum  \\\n",
       "0                                            NaN   \n",
       "1                                            NaN   \n",
       "2                                            NaN   \n",
       "3                                            0.0   \n",
       "4                                            NaN   \n",
       "\n",
       "   previous_NAME_GOODS_CATEGORY_Sport and Leisure_mean  \\\n",
       "0                                                0.0     \n",
       "1                                                0.0     \n",
       "2                                                0.0     \n",
       "3                                                0.0     \n",
       "4                                                0.0     \n",
       "\n",
       "   client_credit_CNT_DRAWINGS_POS_CURRENT_min_sum  \\\n",
       "0                                             NaN   \n",
       "1                                             NaN   \n",
       "2                                             NaN   \n",
       "3                                             0.0   \n",
       "4                                             NaN   \n",
       "\n",
       "   client_credit_NAME_CONTRACT_STATUS_Completed_mean_min  \\\n",
       "0                                                NaN       \n",
       "1                                                NaN       \n",
       "2                                                NaN       \n",
       "3                                                0.0       \n",
       "4                                                NaN       \n",
       "\n",
       "   OCCUPATION_TYPE_Drivers  TARGET  SK_ID_CURR  \n",
       "0                    False    True      100002  \n",
       "1                    False   False      100003  \n",
       "2                    False   False      100004  \n",
       "3                    False   False      100006  \n",
       "4                    False   False      100007  \n",
       "\n",
       "[5 rows x 349 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_reduced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reduced=pd.read_csv('test_reduced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((307511, 349), (48744, 348))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_reduced.shape, test_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reduced=train_reduced.drop(['AMT_CREDIT', 'AMT_ANNUITY', 'CREDIT_TERM'], axis=1)\n",
    "test_reduced=test_reduced.drop(['AMT_CREDIT', 'AMT_ANNUITY', 'CREDIT_TERM'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((307511, 346), (48744, 345))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_reduced.shape, test_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_r, test_r = train_test_split(train_reduced, test_size=0.2, random_state=42)\n",
    "\n",
    "# Extraire les identifiants, les cibles et les caractéristiques pour les ensembles d'entraînement et de test\n",
    "X_train_r = train_r.drop(['SK_ID_CURR', 'TARGET'], axis=1)\n",
    "y_train_r = train_r['TARGET']\n",
    "id_train_r = train_r['SK_ID_CURR']\n",
    "\n",
    "X_test_r = test_r.drop(['SK_ID_CURR', 'TARGET'], axis=1)\n",
    "y_test_r = test_r['TARGET']\n",
    "id_test_r = test_r['SK_ID_CURR']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_r.shape, X_test_r.shape, y_train_r.shape, y_test_r.shape, id_train_r.shape, id_test_r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_modelisation (X_train, X_test) :\n",
    "\n",
    "    # Create a label encoder object\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    # Iterate through the columns and label encode if object type and nunique <=2\n",
    "    for col in X_train.columns:\n",
    "        if X_train[col].dtype == 'object' and len(list(X_train[col].unique())) <= 2:\n",
    "            # Apply the label encoder to both training and test sets\n",
    "            X_train[col] = le.fit_transform(X_train[col])\n",
    "            X_test[col] = le.transform(X_test[col])\n",
    "\n",
    "               \n",
    "    # one-hot encoding of categorical variables\n",
    "    X_train = pd.get_dummies(X_train)\n",
    "    X_test = pd.get_dummies(X_test)\n",
    "\n",
    "    # Assurons-nous que X_train et X_test ont les mêmes colonnes\n",
    "    X_train, X_test = X_train.align(X_test, join='inner', axis=1)\n",
    "    \n",
    "   \n",
    "    # Median imputation of missing values\n",
    "    imputer = SimpleImputer(strategy = 'median')\n",
    "\n",
    "    # Scale each feature to 0-1\n",
    "    scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "\n",
    "    X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)\n",
    "    X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)\n",
    "    X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "    X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "   \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_r, X_test_r=prepare_modelisation (X_train_r, X_test_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_r.shape, X_test_r.shape, y_train_r.shape, y_test_r.shape, id_train_r.shape, id_test_r.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **FONCTION DU NB 1 A SUPPRIMER QUAND ON GROUPERA LES NB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_f1(y_true, y_pred_proba):\n",
    "    thresholds = np.linspace(0, 1, 100)\n",
    "    best_threshold = 0.5\n",
    "    best_f1 = 0\n",
    "    best_cost = float('inf')\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        # Calculer les coûts\n",
    "        fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "        fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "        cost = fn * 10 + fp\n",
    "        \n",
    "        # Sélectionner le seuil basé sur le coût le plus bas\n",
    "        if cost < best_cost:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "            best_cost = cost\n",
    "    \n",
    "    return best_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def custom_recall(y_true, y_pred_proba):\n",
    "    thresholds = np.linspace(0, 1, 100)\n",
    "    best_threshold = 0.5\n",
    "    best_recall = 0\n",
    "    best_cost = float('inf')\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "        recall = recall_score(y_true, y_pred)\n",
    "        # Calculer les coûts\n",
    "        fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "        fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "        cost = fn * 10 + fp\n",
    "        \n",
    "        # Sélectionner le seuil basé sur le coût le plus bas\n",
    "        if cost < best_cost:\n",
    "            best_recall = recall\n",
    "            best_threshold = threshold\n",
    "            best_cost = cost\n",
    "    \n",
    "    return best_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour run la grid search. J'y ajoute le fit_params dans le quel j'ajouterai l'early stopping pour le lgbm\n",
    "\n",
    "def run_grid_search(X_train, y_train, model, param_grid, fit_params=None, train_size=1.0):\n",
    "    # Définir les scorers personnalisés\n",
    "    scorers = {\n",
    "        'AUC': 'roc_auc',\n",
    "        'F1_opt': make_scorer(custom_f1, needs_proba=True),\n",
    "        'recall_opt': make_scorer(custom_recall, needs_proba=True)\n",
    "    }\n",
    "\n",
    "    # Créer un pipeline avec SMOTE et le modèle\n",
    "    pipeline = Pipeline([\n",
    "        ('sampling', SMOTE(random_state=42)),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    # Mettre à jour le param_grid pour correspondre au pipeline\n",
    "    # Les paramètres du modèle doivent être préfixés par 'model__'\n",
    "    param_grid = {f'model__{key}': value for key, value in param_grid.items()}\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid=param_grid,\n",
    "        scoring=scorers,\n",
    "        refit='AUC',\n",
    "        cv=2,\n",
    "        verbose=3,\n",
    "        return_train_score=True\n",
    "    )\n",
    "\n",
    "     # Fractionnement des données si train_size < 1.0\n",
    "    if train_size < 1.0:\n",
    "        X_train_sample, _, y_train_sample, _ = train_test_split(X_train, y_train, train_size=train_size, random_state=42)\n",
    "        data_to_fit = (X_train_sample, y_train_sample)\n",
    "    else:\n",
    "        data_to_fit = (X_train, y_train)\n",
    "\n",
    "    # Entraînement\n",
    "    start_time = time.time()\n",
    "    grid_search.fit(*data_to_fit)\n",
    "    end_time = time.time()\n",
    "\n",
    "    execution_time = round(end_time - start_time, 2)\n",
    "    print(f\"Le temps d'execution est de {execution_time} secondes.\")\n",
    "\n",
    "    return {\n",
    "        'best_model' : grid_search.best_estimator_,\n",
    "        'best_params_': grid_search.best_params_,\n",
    "        'best_score_': grid_search.best_score_,\n",
    "        'cv_results_': grid_search.cv_results_\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_best_auc_result(results):\n",
    "    \"\"\"\n",
    "    Extracts the best model configuration based on AUC score from the cv_results of a GridSearchCV.\n",
    "    \n",
    "    Parameters:\n",
    "    - results (dict): A dictionary output from the run_grid_search function containing 'cv_results_'.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: A DataFrame containing the best scoring row based on the AUC rank.\n",
    "    \"\"\"\n",
    "    # Conversion of cv_results_ into a DataFrame\n",
    "    cv_results = results['cv_results_']\n",
    "    df_cv_results = pd.DataFrame(cv_results)\n",
    "    \n",
    "    # Sorting the DataFrame by the rank of the AUC test scores\n",
    "    df_cv_results = df_cv_results.sort_values(by=\"rank_test_AUC\", ascending=True)\n",
    "    \n",
    "    # Returning the top entry\n",
    "    return df_cv_results.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_auc_scores(best_auc_result):\n",
    "    \"\"\"\n",
    "    Plots the AUC scores from cross-validation folds for the best model configuration based on rank.\n",
    "    \n",
    "    Parameters:\n",
    "    - best_auc_result (DataFrame): A DataFrame with the top row from cv_results_ sorted by AUC.\n",
    "    - full_cv_results (DataFrame): The full cv_results DataFrame to extract mean scores for rank 1.\n",
    "    \"\"\"\n",
    "    # Extract the AUC scores for each fold for the best AUC configuration\n",
    "    df_train_results_auc = best_auc_result[[\n",
    "        'split0_train_AUC', \n",
    "        'split1_train_AUC', \n",
    "        # 'split2_test_AUC', \n",
    "        # 'split3_test_AUC', \n",
    "        # 'split4_test_AUC'\n",
    "    ]][best_auc_result['rank_test_AUC'] == 1].values\n",
    "    \n",
    "    df_test_results_auc = best_auc_result[[\n",
    "        'split0_test_AUC', \n",
    "        'split1_test_AUC', \n",
    "        # 'split2_test_AUC', \n",
    "        # 'split3_test_AUC', \n",
    "        # 'split4_test_AUC'\n",
    "    ]][best_auc_result['rank_test_AUC'] == 1].values\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    \n",
    "    # Plot the train scores\n",
    "    ax.plot(range(0, 2), df_test_results_auc.reshape(-1), label='Scores de validation')\n",
    "    \n",
    "    # Plot the test scores\n",
    "    ax.plot(range(0, 2), df_train_results_auc.reshape(-1), label='Scores de train')\n",
    "\n",
    "    # # Plot the average for the combinations with rank_test_AUC == 1\n",
    "    # mean_scores_rank_1 = best_auc_result.loc[best_auc_result['rank_test_AUC'] == 1, 'mean_test_AUC']\n",
    "    # ax.axhline(y=mean_scores_rank_1.values[0], color='g', linestyle='--', label='Moyenne (rank 1)')\n",
    "    \n",
    "    # Set the properties of the axis\n",
    "    ax.set_xticks(range(0, 2))\n",
    "    ax.set_xlabel(\"Folds de cross-validation\")\n",
    "    ax.set_ylabel('AUC')\n",
    "    ax.set_title(\"AUC de chaque fold \\n pour la combinaison d'hyperparamètres \\n qui arrive au rang 1 en terme d'AUC\", fontsize=15, weight=\"bold\", fontname=\"Impact\", color=\"#0e2452\")\n",
    "    \n",
    "    # Add a legend\n",
    "    ax.legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction qui va établir un seuil pour convertir les probabiliiéts en étiquettes de classe binaire. \n",
    "# à partir de la matrice de confusion, on va calculer le coût\n",
    "\n",
    "def calculate_cost_threshold(y_true, probas, threshold, cost_fn, cost_fp):\n",
    "    y_pred = (probas >= threshold).astype(int) # transforme les probas en prédictions binaires en utilisant un seuil. \n",
    "                                                # Les probabilités >= au seuil sont marquées comme 1 (positives),\n",
    "                                                # les autres comme 0 (négatives).\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel() # .ravel() convertit la matrice 2x2 en un tableau à une dimension [tn, fp, fn, tp]\n",
    "                                                            # permet une extraction facile de chaque valeur\n",
    "    return cost_fn * fn + cost_fp * fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model (X_train, y_train, X_test, y_test, best_model):\n",
    "\n",
    "    start_predict_time = time.time()\n",
    "    # Predict on the training data\n",
    "    y_train_pred_proba = best_model.predict_proba(X_train)[:, 1]\n",
    "    \n",
    "    # Predict on the test data\n",
    "    y_test_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    end_predict_time = time.time()\n",
    "\n",
    "    # Calculate AUC for training and test data\n",
    "    auc_train = round(roc_auc_score(y_train, y_train_pred_proba),2)\n",
    "    auc_test = round(roc_auc_score(y_test, y_test_pred_proba),2)\n",
    "\n",
    "    # Find the optimal threshold for cost function on the training data\n",
    "    thresholds = np.linspace(0, 1, 100)\n",
    "    costs = [calculate_cost_threshold(y_test, y_test_pred_proba, thr, cost_fn=10, cost_fp=1) for thr in thresholds]\n",
    "    optimal_threshold = round(thresholds[np.argmin(costs)],2) \n",
    "\n",
    "    # Metrics at the optimal threshold\n",
    "    y_test_pred_opt = (y_test_pred_proba >= optimal_threshold).astype(int)\n",
    "    \n",
    "    f1_score_1_test = f1_score(y_test, y_test_pred_opt)\n",
    "    recall_1_test = recall_score(y_test, y_test_pred_opt)\n",
    "    accuracy_test = accuracy_score(y_test, y_test_pred_opt)\n",
    "    \n",
    "    return {\n",
    "        'predicting_execution_time': round(end_predict_time - start_predict_time, 2),\n",
    "        'auc_train': auc_train,\n",
    "        'auc_test': auc_test,\n",
    "        'optimal_threshold': optimal_threshold,\n",
    "        'f1_score_1_test': round(f1_score_1_test,2),\n",
    "        'recall_1_test': round(recall_1_test,2),\n",
    "        'accuracy_test': round(accuracy_test,2)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_feature_importances(importances, features):\n",
    "    # Création du DataFrame\n",
    "\n",
    "    importances = importances.flatten()\n",
    "    \n",
    "    feature_importances = pd.DataFrame({\n",
    "        'feature': features, \n",
    "        'importance': importances\n",
    "    })\n",
    "    \n",
    "    # Tri et sélection des 20 caractéristiques les plus importantes\n",
    "    feature_importances_sorted = feature_importances.sort_values(by='importance', ascending=False).reset_index(drop=True)\n",
    "    feature_importances_sorted = feature_importances_sorted.head(20)\n",
    "    \n",
    "    # Normalisation des importances\n",
    "    feature_importances_sorted['importance_normalized'] = feature_importances_sorted['importance'] / feature_importances_sorted['importance'].sum()\n",
    "    \n",
    "    # Création du graphique à barres horizontales\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = plt.subplot()\n",
    "    \n",
    "    # Inversion de l'index pour afficher la plus importante en haut\n",
    "    indices = list(reversed(list(feature_importances_sorted.index)))\n",
    "    ax.barh(indices, feature_importances_sorted['importance_normalized'], align='center', edgecolor='k')\n",
    "    \n",
    "    # Définition des étiquettes y\n",
    "    ax.set_yticks(indices)\n",
    "    ax.set_yticklabels(feature_importances_sorted['feature'])\n",
    "    \n",
    "    # Étiquetage du graphique\n",
    "    plt.xlabel('Normalized Importance')\n",
    "    plt.title('Top 20 Feature Importances')\n",
    "    plt.show()\n",
    "    \n",
    "    return feature_importances_sorted\n",
    "\n",
    "# Exemple d'utilisation\n",
    "# importances = [valeur1, valeur2, ..., valeurN] # remplacez par vos valeurs d'importance réelles\n",
    "# features = ['nom1', 'nom2', ..., 'nomN'] # remplacez par vos noms de caractéristiques réels\n",
    "# feature_importances_sorted = plot_feature_importances(importances, features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **FIN DES FONCTIONS DU NB 1 A SUPPRIMER QUAND ON GROUPERA LES NB**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Nouvelle modélisation LGBM avec features = 95% de l'importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LGBM_2 = lgb.LGBMClassifier(objective='binary',\n",
    "                                boosting_type = 'goss',\n",
    "                                # class_weight = 'balanced',\n",
    "                                random_state = 50)\n",
    "        \n",
    "param_grid_LGBM_2 = {\n",
    "    'num_leaves': [35,40],\n",
    "    'n_estimators' : [150,200],   \n",
    "    'learning_rate' : [0.07, 0.1],\n",
    "}\n",
    "\n",
    "# Paramètres pour early_stopping\n",
    "fit_params = {\n",
    "    'eval_metric': 'auc',\n",
    "    'callbacks': [early_stopping(stopping_rounds=50)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution de la fonction\n",
    "start_training_time = time.time()\n",
    "\n",
    "results_LGBM_2 = run_grid_search(X_train_r, y_train_r, model_LGBM_2, param_grid_LGBM_2,fit_params)\n",
    "\n",
    "end_training_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_execution_time = round(end_training_time - start_training_time,2)  \n",
    "print(f\"Le temps d'exécution est de {training_execution_time} secondes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_lgbm_2 = results_LGBM_2['best_model']\n",
    "best_params_lgbm_2 = results_LGBM_2['best_params_']\n",
    "best_score_lgbm_2 = results_LGBM_2['best_score_']\n",
    "cv_results_lgbm_2 = results_LGBM_2['cv_results_']\n",
    "\n",
    "print(\"Best Params:\", best_params_lgbm_2)\n",
    "print(\"Best AUC:\", best_score_lgbm_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_auc_result_LGBM_2 = extract_best_auc_result(results_LGBM_2)\n",
    "best_auc_result_LGBM_2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auc_scores(best_auc_result_LGBM_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_lgbm_2=evaluate_model (X_train_r, y_train_r, X_test_r, y_test_r, best_model_lgbm_2)\n",
    "result_lgbm_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_lgbm_2.named_steps['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utiliser un échantillon de 10 lignes comme exemple d'entrée\n",
    "input_example = X_train_r.sample(n=10, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Enregistrement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une nouvelle expérience MLflow\n",
    "mlflow.set_experiment(\"MLflow Credit_Scoring - Projet_7\")\n",
    "\n",
    "# Commencer une session MLflow\n",
    "with mlflow.start_run():\n",
    "    # Log les hyperparameters que j'ai déclarés au dessus\n",
    "    mlflow.log_params(best_params_lgbm_2)\n",
    "\n",
    "    # Log les métriques\n",
    "    \n",
    "    mlflow.log_metric(\"optimal_threshold\", result_lgbm_2['optimal_threshold'])\n",
    "\n",
    "    mlflow.log_metric(\"recall_1_test\", result_lgbm_2['recall_1_test'])\n",
    "    mlflow.log_metric(\"f1_score_1_test\", result_lgbm_2['f1_score_1_test'])\n",
    "    mlflow.log_metric(\"accuracy du test\", result_lgbm_2['accuracy_test'])\n",
    "    \n",
    "    mlflow.log_metric(\"auc_train\", result_lgbm_2['auc_train'])\n",
    "    mlflow.log_metric(\"auc_test\", result_lgbm_2['auc_test'])\n",
    "    \n",
    "    mlflow.log_metric(\"temps_fit\", training_execution_time)\n",
    "    mlflow.log_metric(\"temps_predict\", result_lgbm_2['predicting_execution_time'])\n",
    "\n",
    "\n",
    "    # Définir un tag pour se rappeler l'objet de cette session\n",
    "    mlflow.set_tag(\"Training Info\", \"LGBM with 95% importance features\")\n",
    "\n",
    "    # Infer the model signature\n",
    "    signature = infer_signature(X_train_r, best_model_lgbm_2.predict(X_train_r)) # infer_signature génère automatiquement une \"signature\" \n",
    "                                                              # qui décrit les entrées et les sorties du modèle. \n",
    "                                                              # Cela inclut les types de données et les formats attendus par le modèle, \n",
    "                                                              # facilitant ainsi l'intégration et la réutilisation du modèle \n",
    "                                                              # dans différents environnements.\n",
    "\n",
    "    \n",
    "    # Log le model\n",
    "    model_info = mlflow.lightgbm.log_model(\n",
    "        lgb_model=best_model_lgbm_2.named_steps['model'], # le modèle\n",
    "        artifact_path=\"lgbm_classifier_model\", # le chemin où le modèle sera enregistré\n",
    "        signature=signature, # la signature du modèle\n",
    "        input_example=input_example, # un exemple d'input pour montrer comment invoquer le modèle\n",
    "        registered_model_name=\"scoring-credit-lgbm_classifier\", # nom sous lequel le modèle est enregistré dans le registre de modèles MLflow\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Interprétation des résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 - Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = best_model_lgbm_2.named_steps['model'].feature_importances_\n",
    "features= X_train_r.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_r.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_feature_importances(importances, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_sorted=show_feature_importances(importances, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_sorted.to_csv('feature_importances_sorted_lgbm_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 - Shap Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 - Interprétation globale du test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on s'assure que le meilleur modèle est extrait de la pipeline\n",
    "best_model=best_model_lgbm_2.named_steps['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Comme le SMOTE est appliqué dans le pipeline, nous devons transformer les données X_test_r avant d'appliquer SHAP\n",
    "# # Pour cela, nous utilisons le transformateur dans le pipeline avant le modèle\n",
    "# data_for_shap = results_LGBM_2['best_model'].named_steps['sampling'].fit_resample(X_test_r, y_test_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extraction des données transformées qui sont maintenant suréchantillonnées\n",
    "# X_test_resampled, y_test_resampled = data_for_shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de l'Explainer SHAP avec le meilleur modèle obtenu sur le jeu de données de test original\n",
    "explainer = shap.Explainer(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des valeurs SHAP sur les données de test\n",
    "shap_values = explainer.shap_values(X_test_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un graphique récapitulatif SHAP pour l'ensemble des données transformées\n",
    "shap.summary_plot(shap_values, X_test_r, \n",
    "                  plot_type=\"bar\",\n",
    "                 max_display=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si on regarde ce qui se passe pour la classe 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_class1 = shap_values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique récapitulatif SHAP pour la classe 1\n",
    "shap.summary_plot(shap_values_class1, \n",
    "                  X_test_r, \n",
    "                  max_display=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 - Interprétation locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verif\n",
    "train_reduced[train_reduced['SK_ID_CURR']==100002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_r[test_r['SK_ID_CURR']==100002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruction du test\n",
    "# test_dataset = pd.concat([X_test_r, y_test_r.reset_index(drop=True), id_test_r.reset_index(drop=True)], axis=1)\n",
    "# # verif\n",
    "# test_dataset[test_dataset['SK_ID_CURR']==100002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict proba avec le best model\n",
    "y_test_pred_proba = best_model_lgbm_2.predict_proba(X_test_r)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimal threshold for cost function on the training data\n",
    "thresholds = np.linspace(0, 1, 100)\n",
    "costs = [calculate_cost_threshold(y_test_r, y_test_pred_proba, thr, cost_fn=10, cost_fp=1) for thr in thresholds]\n",
    "optimal_threshold = round(thresholds[np.argmin(costs)],2) \n",
    "\n",
    "# Metrics at the optimal threshold\n",
    "y_test_pred_opt = (y_test_pred_proba >= optimal_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_r['target_pred']=y_test_pred_opt\n",
    "test_r['proba']=y_test_pred_proba\n",
    "\n",
    "test_r[test_r['SK_ID_CURR']==100002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(y_test_r, y_test_pred_opt, rownames=['Classes réelles'], colnames=['Classes prédites'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_r, y_test_pred_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2.1 - Interprétation d'un individu prédit en classe 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_r[test_r['target_pred']==1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID de l'individu à analyser\n",
    "specific_id = 384575"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trouver l'index de cet individu dans le jeu de données de test\n",
    "index = test_r[test_r['SK_ID_CURR'] == specific_id].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réinitialiser les index de X_test_r pour obtenir des index consécutifs\n",
    "X_test_r_reset = X_test_r.reset_index(drop=True)\n",
    "\n",
    "# Réinitialiser les index de test_r pour obtenir des index consécutifs\n",
    "test_r_reset = test_r.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette étape assure que les index sont consécutifs et commencent à 0, ce qui facilite l'alignement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trouver le nouvel index relatif dans le jeu de données réinitialisé\n",
    "relative_index = test_r_reset[test_r_reset['SK_ID_CURR'] == specific_id].index[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette ligne de code trouve l'index relatif de l'individu spécifique dans test_r_reset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier les dimensions de shap_values et X_test_r_reset \n",
    "print(\"Dimensions des shap_values pour la classe 1:\", shap_values[1].shape)\n",
    "print(\"Dimensions de X_test_r_reset:\", X_test_r_reset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire les valeurs SHAP pour cet individu spécifique pour la classe 1\n",
    "individual_shap_values = shap_values[1][relative_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le graphique waterfall pour cet individu\n",
    "shap.plots.waterfall(shap.Explanation(values=individual_shap_values,\n",
    "                                      base_values=explainer.expected_value[1],  # Assurez-vous de choisir la bonne valeur de base\n",
    "                                      data=X_test_r_reset.iloc[relative_index],  # Données de l'individu\n",
    "                                      feature_names=X_test_r_reset.columns.tolist()))  # Noms des caractéristiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2.2 - Interprétation d'un individu prédit en classe 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_r[test_r['target_pred']==0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID de l'individu à analyser\n",
    "specific_id = 214010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trouver l'index de cet individu dans le jeu de données de test\n",
    "index = test_r[test_r['SK_ID_CURR'] == specific_id].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réinitialiser les index de X_test_r pour obtenir des index consécutifs\n",
    "X_test_r_reset = X_test_r.reset_index(drop=True)\n",
    "\n",
    "# Réinitialiser les index de test_r pour obtenir des index consécutifs\n",
    "test_r_reset = test_r.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trouver le nouvel index relatif dans le jeu de données réinitialisé\n",
    "relative_index = test_r_reset[test_r_reset['SK_ID_CURR'] == specific_id].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire les valeurs SHAP pour cet individu spécifique pour la classe 0\n",
    "individual_shap_values = shap_values[0][relative_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le graphique waterfall pour cet individu\n",
    "shap.plots.waterfall(shap.Explanation(values=individual_shap_values,\n",
    "                                      base_values=explainer.expected_value[1],  # Assurez-vous de choisir la bonne valeur de base\n",
    "                                      data=X_test_r_reset.iloc[relative_index],  # Données de l'individu\n",
    "                                      feature_names=X_test_r_reset.columns.tolist()))  # Noms des caractéristiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_shap_waterfall(specific_id, shap_values, test_r, explainer, X_test_r):\n",
    "    \"\"\"\n",
    "    Génère un graphique waterfall SHAP pour un individu spécifié par son ID.\n",
    "    \n",
    "    Parameters:\n",
    "    - specific_id : int\n",
    "        L'ID de l'individu pour lequel générer le graphique.\n",
    "    - shap_values : list of numpy arrays\n",
    "        Les valeurs SHAP pour les classes, typiquement [shap_values_class0, shap_values_class1].\n",
    "    - test_r : DataFrame\n",
    "        Le DataFrame contenant les identifiants des individus et les prédictions.\n",
    "    - explainer : shap.Explainer\n",
    "        L'explainer SHAP utilisé pour calculer les valeurs SHAP.\n",
    "    - X_test_r : DataFrame\n",
    "        Le DataFrame des données de test sur lequel les valeurs SHAP ont été calculées.\n",
    "    \"\"\"\n",
    "    # Trouver l'index de l'individu dans le jeu de données de test\n",
    "    index = test_r[test_r['SK_ID_CURR'] == specific_id].index[0]\n",
    "\n",
    "    # Réinitialiser les index de X_test_r pour obtenir des index consécutifs\n",
    "    X_test_r_reset = X_test_r.reset_index(drop=True)\n",
    "\n",
    "    # Réinitialiser les index de test_r pour obtenir des index consécutifs\n",
    "    test_r_reset = test_r.reset_index(drop=True)\n",
    "\n",
    "    # Trouver le nouvel index relatif dans le jeu de données réinitialisé\n",
    "    relative_index = test_r_reset[test_r_reset['SK_ID_CURR'] == specific_id].index[0]\n",
    "\n",
    "    # Extraire les valeurs SHAP pour cet individu spécifique\n",
    "    predicted_class = test_r.loc[test_r.index[relative_index], 'target_pred']\n",
    "    individual_shap_values = shap_values[predicted_class][relative_index]\n",
    "\n",
    "    # Créer le graphique waterfall pour cet individu\n",
    "    shap.plots.waterfall(shap.Explanation(values=individual_shap_values,\n",
    "                                          base_values=explainer.expected_value[predicted_class],  # Assurez-vous de choisir la bonne valeur de base\n",
    "                                          data=X_test_r_reset.iloc[relative_index],  # Données de l'individu\n",
    "                                          feature_names=X_test_r_reset.columns.tolist()))  # Noms des caractéristiques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour un individu prédit en classe 1\n",
    "generate_shap_waterfall(384575, shap_values, test_r, explainer, X_test_r)\n",
    "\n",
    "# Pour un individu prédit en classe 0\n",
    "generate_shap_waterfall(214010, shap_values, test_r, explainer, X_test_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSION : la suppression des variables AMT_CREDIT, AMT_ANNUITY, CREDIT_TERM n'a pas affecté la performance du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la suite, je me suis basée sur : https://app.livestorm.co/openclassrooms-1/deployez-une-api-de-prediction/live?s=021e19b0-cd31-4872-8b6f-b71209d05664#/chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Sérialisation du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sérialisation = le fait de convertir un objet qui est stocké de façon numérique sur notre mémoire ram. et le stocker sous forme de binaire sur notre ordiinateur. la pipeline était dans notre mémoire vive. On veut la stocker sous forme de fichier pour pouvoir la réutiliser plus tard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut sérialser en pickle ou en joblib. Avec Scikit Learn, on recommande joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(best_model_lgbm_2, 'credit_scoring.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 - Déploiement du modèle en local avec Streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut créer un nouveau fichier Python (par exemple app.py) pour le déploiement avec Streamlit. Je l'ai créé et enregistrer sous ce nom : 'app.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'ai systématiquement des erreurs. Ci-dessous toutes les vérif que j'ai faites et c'est ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
